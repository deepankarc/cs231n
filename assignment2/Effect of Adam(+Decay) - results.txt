This file contains the loss from the two CNN networks trained on the CIFAR-10 datasets to analyse the effect of per parameter adaptive learning rates vs PPALR + manual LR decay. Ref. ConvolutionalNetworks.ipyb sec: Inference

The data here is only for adaptive LR, specifically, Adam.

-------------------- ReLU Network --------------------

(Iteration 1 / 3920) loss: 2.295290
(Epoch 0 / 4) train acc: 0.122000; val_acc: 0.147000
(Iteration 21 / 3920) loss: 2.081267
(Iteration 41 / 3920) loss: 1.777724
(Iteration 61 / 3920) loss: 1.553957
(Iteration 81 / 3920) loss: 1.862842
(Iteration 101 / 3920) loss: 1.610387
(Iteration 121 / 3920) loss: 1.239023
(Iteration 141 / 3920) loss: 1.535904
(Iteration 161 / 3920) loss: 1.356673
(Iteration 181 / 3920) loss: 1.354781
(Iteration 201 / 3920) loss: 1.390044
(Iteration 221 / 3920) loss: 1.312091
(Iteration 241 / 3920) loss: 1.246815
(Iteration 261 / 3920) loss: 1.262064
(Iteration 281 / 3920) loss: 1.370474
(Iteration 301 / 3920) loss: 1.289146
(Iteration 321 / 3920) loss: 1.683669
(Iteration 341 / 3920) loss: 1.307996
(Iteration 361 / 3920) loss: 1.248729
(Iteration 381 / 3920) loss: 1.203683
(Iteration 401 / 3920) loss: 1.238655
(Iteration 421 / 3920) loss: 1.180346
(Iteration 441 / 3920) loss: 1.138947
(Iteration 461 / 3920) loss: 1.525084
(Iteration 481 / 3920) loss: 1.195380
(Iteration 501 / 3920) loss: 1.557476
(Iteration 521 / 3920) loss: 1.231897
(Iteration 541 / 3920) loss: 1.132609
(Iteration 561 / 3920) loss: 1.032354
(Iteration 581 / 3920) loss: 1.232879
(Iteration 601 / 3920) loss: 1.207254
(Iteration 621 / 3920) loss: 1.069335
(Iteration 641 / 3920) loss: 1.075927
(Iteration 661 / 3920) loss: 0.974539
(Iteration 681 / 3920) loss: 1.287820
(Iteration 701 / 3920) loss: 0.957801
(Iteration 721 / 3920) loss: 1.211656
(Iteration 741 / 3920) loss: 1.187036
(Iteration 761 / 3920) loss: 1.222123
(Iteration 781 / 3920) loss: 1.262035
(Iteration 801 / 3920) loss: 1.049816
(Iteration 821 / 3920) loss: 1.111034
(Iteration 841 / 3920) loss: 0.911383
(Iteration 861 / 3920) loss: 1.102642
(Iteration 881 / 3920) loss: 1.057896
(Iteration 901 / 3920) loss: 0.908255
(Iteration 921 / 3920) loss: 1.037481
(Iteration 941 / 3920) loss: 0.968412
(Iteration 961 / 3920) loss: 1.424759
(Epoch 1 / 4) train acc: 0.652000; val_acc: 0.596000
(Iteration 981 / 3920) loss: 0.987387
(Iteration 1001 / 3920) loss: 1.068466
(Iteration 1021 / 3920) loss: 1.274411
(Iteration 1041 / 3920) loss: 0.971432
(Iteration 1061 / 3920) loss: 0.942117
(Iteration 1081 / 3920) loss: 0.870436
(Iteration 1101 / 3920) loss: 1.148036
(Iteration 1121 / 3920) loss: 1.086963
(Iteration 1141 / 3920) loss: 0.934446
(Iteration 1161 / 3920) loss: 0.915536
(Iteration 1181 / 3920) loss: 1.062118
(Iteration 1201 / 3920) loss: 1.058884
(Iteration 1221 / 3920) loss: 0.815078
(Iteration 1241 / 3920) loss: 0.980391
(Iteration 1261 / 3920) loss: 0.784186
(Iteration 1281 / 3920) loss: 1.238099
(Iteration 1301 / 3920) loss: 0.895711
(Iteration 1321 / 3920) loss: 0.987539
(Iteration 1341 / 3920) loss: 0.969654
(Iteration 1361 / 3920) loss: 1.008383
(Iteration 1381 / 3920) loss: 1.029355
(Iteration 1401 / 3920) loss: 0.748378
(Iteration 1421 / 3920) loss: 0.943006
(Iteration 1441 / 3920) loss: 0.994680
(Iteration 1461 / 3920) loss: 0.908794
(Iteration 1481 / 3920) loss: 0.932172
(Iteration 1501 / 3920) loss: 0.732304
(Iteration 1521 / 3920) loss: 0.776204
(Iteration 1541 / 3920) loss: 0.868927
(Iteration 1561 / 3920) loss: 1.045198
(Iteration 1581 / 3920) loss: 1.094085
(Iteration 1601 / 3920) loss: 0.571634
(Iteration 1621 / 3920) loss: 0.797931
(Iteration 1641 / 3920) loss: 0.958316
(Iteration 1661 / 3920) loss: 0.799572
(Iteration 1681 / 3920) loss: 0.992114
(Iteration 1701 / 3920) loss: 0.926839
(Iteration 1721 / 3920) loss: 0.779612
(Iteration 1741 / 3920) loss: 1.059692
(Iteration 1761 / 3920) loss: 0.764966
(Iteration 1781 / 3920) loss: 0.931705
(Iteration 1801 / 3920) loss: 0.873200
(Iteration 1821 / 3920) loss: 0.561716
(Iteration 1841 / 3920) loss: 0.733504
(Iteration 1861 / 3920) loss: 0.976125
(Iteration 1881 / 3920) loss: 0.920341
(Iteration 1901 / 3920) loss: 0.932680
(Iteration 1921 / 3920) loss: 0.927626
(Iteration 1941 / 3920) loss: 0.810333
(Epoch 2 / 4) train acc: 0.757000; val_acc: 0.642000
(Iteration 1961 / 3920) loss: 0.845547
(Iteration 1981 / 3920) loss: 0.931173
(Iteration 2001 / 3920) loss: 0.854307
(Iteration 2021 / 3920) loss: 0.838948
(Iteration 2041 / 3920) loss: 0.956421
(Iteration 2061 / 3920) loss: 0.842566
(Iteration 2081 / 3920) loss: 0.824261
(Iteration 2101 / 3920) loss: 0.820969
(Iteration 2121 / 3920) loss: 0.803332
(Iteration 2141 / 3920) loss: 0.917551
(Iteration 2161 / 3920) loss: 0.600358
(Iteration 2181 / 3920) loss: 0.730307
(Iteration 2201 / 3920) loss: 0.757703
(Iteration 2221 / 3920) loss: 0.670733
(Iteration 2241 / 3920) loss: 0.726931
(Iteration 2261 / 3920) loss: 0.572745
(Iteration 2281 / 3920) loss: 0.661693
(Iteration 2301 / 3920) loss: 0.970080
(Iteration 2321 / 3920) loss: 0.605603
(Iteration 2341 / 3920) loss: 0.850997
(Iteration 2361 / 3920) loss: 0.732225
(Iteration 2381 / 3920) loss: 0.736237
(Iteration 2401 / 3920) loss: 0.645944
(Iteration 2421 / 3920) loss: 1.073775
(Iteration 2441 / 3920) loss: 0.746455
(Iteration 2461 / 3920) loss: 0.697746
(Iteration 2481 / 3920) loss: 0.565741
(Iteration 2501 / 3920) loss: 0.597725
(Iteration 2521 / 3920) loss: 0.543292
(Iteration 2541 / 3920) loss: 0.608756
(Iteration 2561 / 3920) loss: 0.624325
(Iteration 2581 / 3920) loss: 0.569660
(Iteration 2601 / 3920) loss: 0.821996
(Iteration 2621 / 3920) loss: 0.590409
(Iteration 2641 / 3920) loss: 0.879379
(Iteration 2661 / 3920) loss: 1.026519
(Iteration 2681 / 3920) loss: 0.703186
(Iteration 2701 / 3920) loss: 0.603250
(Iteration 2721 / 3920) loss: 0.500028
(Iteration 2741 / 3920) loss: 0.607566
(Iteration 2761 / 3920) loss: 0.675614
(Iteration 2781 / 3920) loss: 0.591154
(Iteration 2801 / 3920) loss: 0.757066
(Iteration 2821 / 3920) loss: 0.618558
(Iteration 2841 / 3920) loss: 0.680560
(Iteration 2861 / 3920) loss: 0.770632
(Iteration 2881 / 3920) loss: 0.661649
(Iteration 2901 / 3920) loss: 0.605787
(Iteration 2921 / 3920) loss: 0.508863
(Epoch 3 / 4) train acc: 0.790000; val_acc: 0.667000
(Iteration 2941 / 3920) loss: 0.595008
(Iteration 2961 / 3920) loss: 0.602811
(Iteration 2981 / 3920) loss: 0.517779
(Iteration 3001 / 3920) loss: 0.806173
(Iteration 3021 / 3920) loss: 0.545502
(Iteration 3041 / 3920) loss: 0.821055
(Iteration 3061 / 3920) loss: 0.653431
(Iteration 3081 / 3920) loss: 0.707460
(Iteration 3101 / 3920) loss: 0.605969
(Iteration 3121 / 3920) loss: 0.622221
(Iteration 3141 / 3920) loss: 0.678283
(Iteration 3161 / 3920) loss: 0.638134
(Iteration 3181 / 3920) loss: 0.625333
(Iteration 3201 / 3920) loss: 0.586391
(Iteration 3221 / 3920) loss: 0.619113
(Iteration 3241 / 3920) loss: 0.703566
(Iteration 3261 / 3920) loss: 0.863703
(Iteration 3281 / 3920) loss: 0.467878
(Iteration 3301 / 3920) loss: 0.705882
(Iteration 3321 / 3920) loss: 0.406021
(Iteration 3341 / 3920) loss: 0.717838
(Iteration 3361 / 3920) loss: 0.649778
(Iteration 3381 / 3920) loss: 0.665707
(Iteration 3401 / 3920) loss: 0.722679
(Iteration 3421 / 3920) loss: 0.538154
(Iteration 3441 / 3920) loss: 0.563692
(Iteration 3461 / 3920) loss: 0.536169
(Iteration 3481 / 3920) loss: 0.609482
(Iteration 3501 / 3920) loss: 0.528307
(Iteration 3521 / 3920) loss: 0.485635
(Iteration 3541 / 3920) loss: 0.717500
(Iteration 3561 / 3920) loss: 0.548069
(Iteration 3581 / 3920) loss: 0.600403
(Iteration 3601 / 3920) loss: 0.648944
(Iteration 3621 / 3920) loss: 0.505994
(Iteration 3641 / 3920) loss: 0.464766
(Iteration 3661 / 3920) loss: 0.474246
(Iteration 3681 / 3920) loss: 0.698578
(Iteration 3701 / 3920) loss: 0.410813
(Iteration 3721 / 3920) loss: 0.513865
(Iteration 3741 / 3920) loss: 0.710498
(Iteration 3761 / 3920) loss: 0.700423
(Iteration 3781 / 3920) loss: 0.434023
(Iteration 3801 / 3920) loss: 0.568234
(Iteration 3821 / 3920) loss: 0.624613
(Iteration 3841 / 3920) loss: 0.379882
(Iteration 3861 / 3920) loss: 0.556881
(Iteration 3881 / 3920) loss: 0.672396
(Iteration 3901 / 3920) loss: 0.576430
(Epoch 4 / 4) train acc: 0.852000; val_acc: 0.701000


-------------------- PReLU Network --------------------

(Iteration 1 / 4900) loss: 2.303904
(Epoch 0 / 5) train acc: 0.209000; val_acc: 0.219000
(Iteration 21 / 4900) loss: 1.944907
(Iteration 41 / 4900) loss: 1.781851
(Iteration 61 / 4900) loss: 1.686447
(Iteration 81 / 4900) loss: 1.669513
(Iteration 101 / 4900) loss: 1.371355
(Iteration 121 / 4900) loss: 1.292781
(Iteration 141 / 4900) loss: 1.695272
(Iteration 161 / 4900) loss: 1.435815
(Iteration 181 / 4900) loss: 1.431982
(Iteration 201 / 4900) loss: 1.387916
(Iteration 221 / 4900) loss: 1.392358
(Iteration 241 / 4900) loss: 1.368391
(Iteration 261 / 4900) loss: 1.485953
(Iteration 281 / 4900) loss: 1.353490
(Iteration 301 / 4900) loss: 1.299001
(Iteration 321 / 4900) loss: 1.213693
(Iteration 341 / 4900) loss: 1.496626
(Iteration 361 / 4900) loss: 1.329762
(Iteration 381 / 4900) loss: 1.196643
(Iteration 401 / 4900) loss: 1.166689
(Iteration 421 / 4900) loss: 1.293260
(Iteration 441 / 4900) loss: 1.073640
(Iteration 461 / 4900) loss: 1.326169
(Iteration 481 / 4900) loss: 1.349958
(Iteration 501 / 4900) loss: 1.317449
(Iteration 521 / 4900) loss: 1.278763
(Iteration 541 / 4900) loss: 0.832529
(Iteration 561 / 4900) loss: 1.125599
(Iteration 581 / 4900) loss: 1.225898
(Iteration 601 / 4900) loss: 1.069085
(Iteration 621 / 4900) loss: 1.221017
(Iteration 641 / 4900) loss: 1.268544
(Iteration 661 / 4900) loss: 1.149381
(Iteration 681 / 4900) loss: 1.127683
(Iteration 701 / 4900) loss: 0.971896
(Iteration 721 / 4900) loss: 1.240579
(Iteration 741 / 4900) loss: 0.930059
(Iteration 761 / 4900) loss: 1.176993
(Iteration 781 / 4900) loss: 1.092730
(Iteration 801 / 4900) loss: 1.153583
(Iteration 821 / 4900) loss: 0.956407
(Iteration 841 / 4900) loss: 1.503516
(Iteration 861 / 4900) loss: 1.021253
(Iteration 881 / 4900) loss: 1.134833
(Iteration 901 / 4900) loss: 0.875942
(Iteration 921 / 4900) loss: 0.964323
(Iteration 941 / 4900) loss: 1.110779
(Iteration 961 / 4900) loss: 1.102371
(Epoch 1 / 5) train acc: 0.640000; val_acc: 0.606000
(Iteration 981 / 4900) loss: 1.084078
(Iteration 1001 / 4900) loss: 1.209233
(Iteration 1021 / 4900) loss: 0.873818
(Iteration 1041 / 4900) loss: 1.275843
(Iteration 1061 / 4900) loss: 1.198919
(Iteration 1081 / 4900) loss: 1.002371
(Iteration 1101 / 4900) loss: 1.024879
(Iteration 1121 / 4900) loss: 0.871611
(Iteration 1141 / 4900) loss: 0.968240
(Iteration 1161 / 4900) loss: 1.108603
(Iteration 1181 / 4900) loss: 1.088480
(Iteration 1201 / 4900) loss: 0.794196
(Iteration 1221 / 4900) loss: 0.669172
(Iteration 1241 / 4900) loss: 0.714177
(Iteration 1261 / 4900) loss: 0.693417
(Iteration 1281 / 4900) loss: 1.223811
(Iteration 1301 / 4900) loss: 0.889722
(Iteration 1321 / 4900) loss: 1.040439
(Iteration 1341 / 4900) loss: 1.179257
(Iteration 1361 / 4900) loss: 1.059249
(Iteration 1381 / 4900) loss: 1.215884
(Iteration 1401 / 4900) loss: 1.169546
(Iteration 1421 / 4900) loss: 0.803806
(Iteration 1441 / 4900) loss: 1.090735
(Iteration 1461 / 4900) loss: 0.788717
(Iteration 1481 / 4900) loss: 0.905497
(Iteration 1501 / 4900) loss: 1.056237
(Iteration 1521 / 4900) loss: 0.906512
(Iteration 1541 / 4900) loss: 0.800090
(Iteration 1561 / 4900) loss: 1.071792
(Iteration 1581 / 4900) loss: 0.839464
(Iteration 1601 / 4900) loss: 0.781039
(Iteration 1621 / 4900) loss: 0.924258
(Iteration 1641 / 4900) loss: 0.896123
(Iteration 1661 / 4900) loss: 0.992421
(Iteration 1681 / 4900) loss: 0.969194
(Iteration 1701 / 4900) loss: 0.820802
(Iteration 1721 / 4900) loss: 0.887174
(Iteration 1741 / 4900) loss: 0.856889
(Iteration 1761 / 4900) loss: 0.862728
(Iteration 1781 / 4900) loss: 0.885736
(Iteration 1801 / 4900) loss: 0.814792
(Iteration 1821 / 4900) loss: 0.706530
(Iteration 1841 / 4900) loss: 0.825914
(Iteration 1861 / 4900) loss: 0.941055
(Iteration 1881 / 4900) loss: 0.839314
(Iteration 1901 / 4900) loss: 0.845388
(Iteration 1921 / 4900) loss: 0.816792
(Iteration 1941 / 4900) loss: 0.658829
(Epoch 2 / 5) train acc: 0.756000; val_acc: 0.653000
(Iteration 1961 / 4900) loss: 0.802554
(Iteration 1981 / 4900) loss: 1.023179
(Iteration 2001 / 4900) loss: 0.728926
(Iteration 2021 / 4900) loss: 0.643173
(Iteration 2041 / 4900) loss: 0.923122
(Iteration 2061 / 4900) loss: 0.845517
(Iteration 2081 / 4900) loss: 0.632265
(Iteration 2101 / 4900) loss: 0.729547
(Iteration 2121 / 4900) loss: 0.868874
(Iteration 2141 / 4900) loss: 0.662944
(Iteration 2161 / 4900) loss: 0.680607
(Iteration 2181 / 4900) loss: 0.773959
(Iteration 2201 / 4900) loss: 0.678602
(Iteration 2221 / 4900) loss: 0.502747
(Iteration 2241 / 4900) loss: 0.927259
(Iteration 2261 / 4900) loss: 0.590792
(Iteration 2281 / 4900) loss: 0.805743
(Iteration 2301 / 4900) loss: 0.880779
(Iteration 2321 / 4900) loss: 0.479448
(Iteration 2341 / 4900) loss: 0.890017
(Iteration 2361 / 4900) loss: 0.658472
(Iteration 2381 / 4900) loss: 0.640533
(Iteration 2401 / 4900) loss: 0.591847
(Iteration 2421 / 4900) loss: 0.649156
(Iteration 2441 / 4900) loss: 0.513542
(Iteration 2461 / 4900) loss: 0.705983
(Iteration 2481 / 4900) loss: 0.820141
(Iteration 2501 / 4900) loss: 0.426644
(Iteration 2521 / 4900) loss: 0.842200
(Iteration 2541 / 4900) loss: 0.606868
(Iteration 2561 / 4900) loss: 0.775788
(Iteration 2581 / 4900) loss: 0.577766
(Iteration 2601 / 4900) loss: 0.711632
(Iteration 2621 / 4900) loss: 0.681308
(Iteration 2641 / 4900) loss: 0.526264
(Iteration 2661 / 4900) loss: 0.633634
(Iteration 2681 / 4900) loss: 0.451725
(Iteration 2701 / 4900) loss: 0.541449
(Iteration 2721 / 4900) loss: 0.605471
(Iteration 2741 / 4900) loss: 0.863692
(Iteration 2761 / 4900) loss: 0.691903
(Iteration 2781 / 4900) loss: 0.711802
(Iteration 2801 / 4900) loss: 0.517319
(Iteration 2821 / 4900) loss: 0.642090
(Iteration 2841 / 4900) loss: 0.782396
(Iteration 2861 / 4900) loss: 0.851859
(Iteration 2881 / 4900) loss: 0.554329
(Iteration 2901 / 4900) loss: 0.518485
(Iteration 2921 / 4900) loss: 0.552221
(Epoch 3 / 5) train acc: 0.787000; val_acc: 0.687000
(Iteration 2941 / 4900) loss: 0.696960
(Iteration 2961 / 4900) loss: 0.807211
(Iteration 2981 / 4900) loss: 0.555764
(Iteration 3001 / 4900) loss: 0.605394
(Iteration 3021 / 4900) loss: 0.532281
(Iteration 3041 / 4900) loss: 0.362133
(Iteration 3061 / 4900) loss: 0.525563
(Iteration 3081 / 4900) loss: 0.456997
(Iteration 3101 / 4900) loss: 0.731690
(Iteration 3121 / 4900) loss: 0.733313
(Iteration 3141 / 4900) loss: 0.701616
(Iteration 3161 / 4900) loss: 0.499005
(Iteration 3181 / 4900) loss: 0.481978
(Iteration 3201 / 4900) loss: 0.648027
(Iteration 3221 / 4900) loss: 0.700995
(Iteration 3241 / 4900) loss: 0.470602
(Iteration 3261 / 4900) loss: 0.518294
(Iteration 3281 / 4900) loss: 0.618419
(Iteration 3301 / 4900) loss: 0.899939
(Iteration 3321 / 4900) loss: 0.434080
(Iteration 3341 / 4900) loss: 0.604185
(Iteration 3361 / 4900) loss: 0.600599
(Iteration 3381 / 4900) loss: 0.540786
(Iteration 3401 / 4900) loss: 0.522261
(Iteration 3421 / 4900) loss: 0.492509
(Iteration 3441 / 4900) loss: 0.836348
(Iteration 3461 / 4900) loss: 0.579986
(Iteration 3481 / 4900) loss: 0.664916
(Iteration 3501 / 4900) loss: 0.491061
(Iteration 3521 / 4900) loss: 0.570164
(Iteration 3541 / 4900) loss: 0.522923
(Iteration 3561 / 4900) loss: 0.347301
(Iteration 3581 / 4900) loss: 0.467818
(Iteration 3601 / 4900) loss: 0.571602
(Iteration 3621 / 4900) loss: 0.340301
(Iteration 3641 / 4900) loss: 0.371341
(Iteration 3661 / 4900) loss: 0.479087
(Iteration 3681 / 4900) loss: 0.385229
(Iteration 3701 / 4900) loss: 0.623450
(Iteration 3721 / 4900) loss: 0.476114
(Iteration 3741 / 4900) loss: 0.375232
(Iteration 3761 / 4900) loss: 0.432423
(Iteration 3781 / 4900) loss: 0.352618
(Iteration 3801 / 4900) loss: 0.585977
(Iteration 3821 / 4900) loss: 0.414075
(Iteration 3841 / 4900) loss: 0.399297
(Iteration 3861 / 4900) loss: 0.382383
(Iteration 3881 / 4900) loss: 0.525290
(Iteration 3901 / 4900) loss: 0.289464
(Epoch 4 / 5) train acc: 0.892000; val_acc: 0.700000
(Iteration 3921 / 4900) loss: 0.555017
(Iteration 3941 / 4900) loss: 0.539348
(Iteration 3961 / 4900) loss: 0.445696
(Iteration 3981 / 4900) loss: 0.438737
(Iteration 4001 / 4900) loss: 0.472029
(Iteration 4021 / 4900) loss: 0.443751
(Iteration 4041 / 4900) loss: 0.335399
(Iteration 4061 / 4900) loss: 0.383892
(Iteration 4081 / 4900) loss: 0.540746
(Iteration 4101 / 4900) loss: 0.502998
(Iteration 4121 / 4900) loss: 0.436240
(Iteration 4141 / 4900) loss: 0.349255
(Iteration 4161 / 4900) loss: 0.299641
(Iteration 4181 / 4900) loss: 0.441332
(Iteration 4201 / 4900) loss: 0.330489
(Iteration 4221 / 4900) loss: 0.581648
(Iteration 4241 / 4900) loss: 0.315264
(Iteration 4261 / 4900) loss: 0.329273
(Iteration 4281 / 4900) loss: 0.377558
(Iteration 4301 / 4900) loss: 0.595529
(Iteration 4321 / 4900) loss: 0.735096
(Iteration 4341 / 4900) loss: 0.307525
(Iteration 4361 / 4900) loss: 0.256224
(Iteration 4381 / 4900) loss: 0.281567
(Iteration 4401 / 4900) loss: 0.434636
(Iteration 4421 / 4900) loss: 0.350792
(Iteration 4441 / 4900) loss: 0.256810
(Iteration 4461 / 4900) loss: 0.249742
(Iteration 4481 / 4900) loss: 0.304610
(Iteration 4501 / 4900) loss: 0.345676
(Iteration 4521 / 4900) loss: 0.400532
(Iteration 4541 / 4900) loss: 0.345230
(Iteration 4561 / 4900) loss: 0.611092
(Iteration 4581 / 4900) loss: 0.340760
(Iteration 4601 / 4900) loss: 0.238660
(Iteration 4621 / 4900) loss: 0.337734
(Iteration 4641 / 4900) loss: 0.506759
(Iteration 4661 / 4900) loss: 0.294016
(Iteration 4681 / 4900) loss: 0.420224
(Iteration 4701 / 4900) loss: 0.287676
(Iteration 4721 / 4900) loss: 0.387560
(Iteration 4741 / 4900) loss: 0.259370
(Iteration 4761 / 4900) loss: 0.297803
(Iteration 4781 / 4900) loss: 0.253944
(Iteration 4801 / 4900) loss: 0.396967
(Iteration 4821 / 4900) loss: 0.349925
(Iteration 4841 / 4900) loss: 0.446347
(Iteration 4861 / 4900) loss: 0.305563
(Iteration 4881 / 4900) loss: 0.325795
(Epoch 5 / 5) train acc: 0.926000; val_acc: 0.700000